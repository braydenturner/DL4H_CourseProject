{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3a4738-2e3b-4230-a760-5aa18887dbdd",
   "metadata": {},
   "source": [
    "# Deep Learning For Healthcare Course Project: INPREM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bee9d-8169-434e-826e-2e4c9b29c687",
   "metadata": {},
   "source": [
    "https://www.kdd.org/kdd2020/accepted-papers/view/inprem-an-interpretable-and-trustworthy-predictive-model-for-healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47c1bd-157c-444a-be21-ebc3cdbd4645",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5063b31c-2e09-4c90-933d-34354f6fb5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sparsemax in /opt/conda/lib/python3.8/site-packages (0.1.9)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from sparsemax) (1.11.0a0+bfe5ad2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->sparsemax) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U sparsemax\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3dc9044-b4f4-4504-99a1-041a4d89e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import psutil\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from icd9 import ICD9\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsemax import Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc863de-4dea-4fe9-b07c-f02da06a2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU found\n",
      "using patient data\n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# validate GPU usage\n",
    "print(\"using GPU\") if torch.cuda.is_available() else print(\"no GPU found\")\n",
    "\n",
    "# define data path\n",
    "use_demo = False\n",
    "if use_demo:\n",
    "    DATA_PATH = \"demodata/\" # work with open source data\n",
    "    print(\"using demo data\")\n",
    "else:\n",
    "    DATA_PATH = \"data/\" # work with certified patient data\n",
    "    print(\"using patient data\")\n",
    "\n",
    "tree = ICD9('codes.json')\n",
    "# tree.find('001.1').parent.parent.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b317c7b-cdec-477b-ab90-f5004336588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMISSIONS.csv      DIAGNOSES_ICD.csv   D_ICD_DIAGNOSES.csv ICUSTAYS.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59820ba-ebce-486b-8c10-013ffb6615cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafcd45-05f9-4855-89dc-c9f3afe2a3c9",
   "metadata": {},
   "source": [
    "For example, SUBJECT_ID refers to a unique patient, HADM_ID refers to a unique admission to the hospital, and ICUSTAY_ID refers to a unique admission to an intensive care unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d39bac-2632-4d00-9982-d3d5c915774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag_icd (651047 lines):\n",
      "    hadm_id icd9_code\n",
      "0   172335     40301\n",
      "1   172335       486\n",
      "2   172335     58281\n",
      "3   172335      5855\n",
      "4   172335      4254\n",
      "\n",
      "icustays (61532 lines):\n",
      "    subject_id  hadm_id  icustay_id              outtime\n",
      "0         268   110404      280836  2198-02-18 05:26:11\n",
      "1         269   106296      206613  2170-11-08 17:46:57\n",
      "2         270   188028      220345  2128-06-27 12:32:29\n",
      "3         271   173727      249196  2120-08-10 00:39:04\n",
      "4         272   164716      210407  2186-12-27 12:01:13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def convert_datetime_to_day(df):\n",
    "    temp = pd.DataFrame()\n",
    "    temp[\"date\"] = pd.to_datetime(df['outtime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(temp['date'].dt.year) + str(temp['date'].dt.month) + str(temp['date'].dt.day)\n",
    "\n",
    "diag_icd = load_dataset(os.path.join(DATA_PATH, 'DIAGNOSES_ICD.csv'))\n",
    "icd_descriptions = load_dataset(os.path.join(DATA_PATH, 'D_ICD_DIAGNOSES.csv'))\n",
    "icustays = load_dataset(os.path.join(DATA_PATH, 'ICUSTAYS.csv'))\n",
    "admissions = load_dataset(os.path.join(DATA_PATH, 'ADMISSIONS.csv'))\n",
    "\n",
    "diag_icd = diag_icd.rename(columns={\"hadm_id\".upper(): \"hadm_id\", \"icd9_code\".upper(): \"icd9_code\"})\n",
    "icustays = icustays.rename(columns={\"subject_id\".upper(): \"subject_id\", \"hadm_id\".upper(): \"hadm_id\", \"icustay_id\".upper(): \"icustay_id\", \"outtime\".upper(): \"outtime\"})\n",
    "\n",
    "diag_icd = diag_icd[[\"hadm_id\", \"icd9_code\"]]\n",
    "icustays = icustays[[\"subject_id\", \"hadm_id\", \"icustay_id\", \"outtime\"]]\n",
    "\n",
    "\n",
    "print(f\"diag_icd ({len(diag_icd)} lines):\\n\", diag_icd.head(), end=\"\\n\\n\")\n",
    "print(f\"icustays ({len(icustays)} lines):\\n\", icustays.head(), end=\"\\n\\n\")\n",
    "# print(f\"admissions ({admissions.size} lines):\\n\", admissions.head(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12ade38-be99-44c0-9fb7-e5c2dc2b1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_df (705921 lines):\n",
      "   icd9_code  subject_id  icustay_id              outtime\n",
      "0     40301         109      262652  2141-09-22 21:44:50\n",
      "1       486         109      262652  2141-09-22 21:44:50\n",
      "2     58281         109      262652  2141-09-22 21:44:50\n",
      "3      5855         109      262652  2141-09-22 21:44:50\n",
      "4      4254         109      262652  2141-09-22 21:44:50\n"
     ]
    }
   ],
   "source": [
    "joined_df = pd.merge(diag_icd, icustays, how='inner', on='hadm_id')[[\"icd9_code\", \"subject_id\", \"icustay_id\", \"outtime\"]]\n",
    "print(f\"joined_df ({len(joined_df)} lines):\\n\", joined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f3438-5929-4f94-90fd-50e2af5b69b3",
   "metadata": {},
   "source": [
    "Convert y in to category labels. If (3, 4 ,5) always use left 3. If it starts with E, use (Exxx). If it starts with V, use (Vxx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c847451-714f-4bd3-b80a-a37a497105c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8755 patients\n"
     ]
    }
   ],
   "source": [
    "def convert_codes(codes):\n",
    "    out = []\n",
    "    for code in codes:\n",
    "        code = str(code)\n",
    "        if code[0] == \"E\":\n",
    "            c = code[:4]\n",
    "        else:\n",
    "            c = code[:3]\n",
    "        out.append(c)\n",
    "        \n",
    "        unique_codes.add(c)\n",
    "       \n",
    "    return out\n",
    "\n",
    "\n",
    "def build_dictionaries(codes): # use our codes (kept getting keyError)\n",
    "    \"\"\"Construct dicts to map/ reverse map string input codes to keys, e.g. {'001': 0, '002': 1}\n",
    "    \"\"\"\n",
    "    types = dict((diag, idx) for idx, diag in enumerate(codes))\n",
    "    rtypes = dict((idx, diag) for idx, diag in enumerate(codes))\n",
    "    \n",
    "    return types, rtypes\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "all_codes = []\n",
    "unique_codes = set()\n",
    "\n",
    "for name, patient in joined_df.sort_values(\"outtime\").groupby([\"subject_id\"]):\n",
    "    visits = []\n",
    "    for _, visit in patient.groupby([\"icustay_id\"]):\n",
    "        codes = visit[\"icd9_code\"].tolist()\n",
    "        codes = convert_codes(codes)\n",
    "        visits.append(codes)\n",
    "    if len(visits) >= 2:\n",
    "        x, y_ = visits[:-1], visits[-1]\n",
    "        X.append(x)\n",
    "        y.append(y_)\n",
    "\n",
    "types, rtypes = build_dictionaries(list(unique_codes))\n",
    "print(f\"Using {len(X)} patients\")\n",
    "\n",
    "# check mapping\n",
    "# print(unique_codes)\n",
    "# print('diag mapping for DIAG_V10:', types['V10']) # 75\n",
    "# print('reverse mapping for index 75:', rtypes[75])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79702804-810c-41c6-9381-fb1681ac3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X) == len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f80811-2ae9-44bf-81bc-77b08f9edabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071 codes\n",
      "1071 types mappings\n"
     ]
    }
   ],
   "source": [
    "# print(\"visits (x):\", X, \"\\n\")\n",
    "# print(\"last visit (y):\", y)\n",
    "\n",
    "print(len(unique_codes), \"codes\") \n",
    "print(len(types), \"types mappings\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f406311-1f41-41f0-86dc-2d09a286533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD 9 Codes for Binary Classification\n",
    "diabetes = (\"Diabetes\", \"250.xx\")\n",
    "heart_failure = (\"Heary Failure\", \"428.xx\")\n",
    "chronic_kidney_disease = (\"Chronic Kidney Disease\", \"585.xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da5af8",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7071d",
   "metadata": {},
   "source": [
    "For each task, we randomly split each dataset into training, validation, and testing sets five times in a 75:10:15 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8b0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.75\n",
    "\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "test_size = 0.6 # (valid is 10% of remaining 25%, test is 15% of remaining 25%)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf4afc-5be4-4ac2-831a-60d6eaa52c10",
   "metadata": {},
   "source": [
    "## Build Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3895e91d-962b-40de-852d-afb900d64b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "        \n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_valid, y_valid)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f5e7e-a159-45df-a748-97b3c904c3fc",
   "metadata": {},
   "source": [
    "## Load the Data (DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46fda0-2d1c-4529-a191-8f3f190e1b57",
   "metadata": {},
   "source": [
    "For each task, we randomly split each dataset into training, validation, and testing sets five times in a 75:10:15 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784b31cb-ac66-46e2-80dd-b0d9813a973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_retain(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patients) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    y = torch.zeros((len(labels), len(types)), dtype=torch.bool)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # create one-hot vector\n",
    "        for l in label:\n",
    "            plc = types[l]\n",
    "            y[i][plc] = True\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        count = 0\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            visit_len = len(visit)\n",
    "            \n",
    "            typed_visit = visit.copy()\n",
    "            for idx, code in enumerate(visit): # convert to mapping\n",
    "                typed_visit[idx] = types[visit[idx]]\n",
    "            \n",
    "            x[i_patient][j_visit][:visit_len] = torch.tensor(typed_visit, dtype=torch.long)\n",
    "            masks[i_patient][j_visit][:visit_len] = torch.ones((visit_len),dtype=torch.bool)\n",
    "            count += 1\n",
    "            \n",
    "        reverse_x = x[i_patient][:count]\n",
    "        reverse_mask = masks[i_patient][:count]\n",
    "        \n",
    "        rev_x[i_patient][:count] = torch.flip(reverse_x, [0])\n",
    "        rev_masks[i_patient][:count] = torch.flip(reverse_mask, [0])\n",
    "        \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81276f8-6ccc-4e9b-bc7d-bc16d43dbe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071 40\n"
     ]
    }
   ],
   "source": [
    "num_visits = [len(patient) for patient in X]\n",
    "\n",
    "max_num_visits = max(num_visits)\n",
    "max_num_codes = len(types)\n",
    "\n",
    "def collate_fn_inprem(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "    num_patients = len(sequences)\n",
    "\n",
    "    num_types = len(types)\n",
    "    max_num_codes = num_types\n",
    "    \n",
    "    y = torch.zeros((len(labels), num_types), dtype=torch.bool)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # create one-hot vector\n",
    "        for l in label:\n",
    "            plc = types[l]\n",
    "            y[i][plc] = True\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, num_types), dtype=torch.long)\n",
    "    o = torch.zeros((num_patients, max_num_visits), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, num_types), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        count = 1\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            for code in visit : # convert to mapping\n",
    "                plc = types[code]\n",
    "                x[i_patient][j_visit][plc] = True\n",
    "            o[i_patient][j_visit] = count\n",
    "            count += 1\n",
    "    \n",
    "    return x, o, y\n",
    "\n",
    "print(max_num_codes, max_num_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d03a1e-7bd8-467c-b155-b1ab699ce8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, test_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn_inprem, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn_inprem)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn_inprem)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn_inprem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c7b45-5f82-4852-8ad2-a7775c775d75",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee0171-82a3-4f49-8baa-fef7e1f5efee",
   "metadata": {},
   "source": [
    "We treat the medical events taking place in EHR as medical codes, which are denoted as $c_{1}, c_{2},... c_{|C|}$ ‚àà ùê∂, where |ùê∂| is the total number of unique medical codes.\n",
    "\n",
    "One specific patient consist of a sequence of visits $v_{1}, v_{2},... v_{T}$ where we denote the number of visits in total as T.\n",
    "\n",
    "Each visit contains a subset of medical codes, and we denote each visit as a binary vector  $v_{t} ‚àà \\{0, 1\\}_{|C|}$, where the ùëñ-th element is set to 1 if the ùë°-th visit contains the medical code $c_{i}$, otherwise 0. The visits  $v_{1}, v_{2},... v_{T}$ are stacked to form an input matrix $X ‚àà \\{0, 1\\}^{|C|xT}$ , which we use as the input for the network\n",
    "\n",
    "$E_{v} = {W}_{v}X$\n",
    "\n",
    "$E_{o} = {W}_{o}O$\n",
    "\n",
    "$E_{r} = \\alpha(\\beta \\odot (E_{v}+E_{o}))^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c92abe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Retain (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae07459",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AlphaAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(probs\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# load the model here\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m retain \u001b[38;5;241m=\u001b[39m \u001b[43mRETAIN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_codes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m retain\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mRETAIN.__init__\u001b[0;34m(self, num_codes, embedding_dim)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_a \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGRU(embedding_dim, embedding_dim, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_b \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGRU(embedding_dim, embedding_dim, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_a \u001b[38;5;241m=\u001b[39m \u001b[43mAlphaAttention\u001b[49m(embedding_dim)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_b \u001b[38;5;241m=\u001b[39m BetaAttention(embedding_dim)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embedding_dim, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AlphaAttention' is not defined"
     ]
    }
   ],
   "source": [
    "class RETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_v = nn.Embedding(num_codes, embedding_dim)\n",
    "        self.embedding_o = nn.Embedding(num_codes, embedding_dim)\n",
    "        \n",
    "        self.rnn_a = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.rnn_b = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n",
    "        \n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.do = nn.Dropout(.5)\n",
    "    \n",
    "    def forward(self, X, masks, rev_X, rev_masks):\n",
    "    \n",
    "        rev_X = self.embedding_v(rev_X)\n",
    "        rev_X = self.embedding_o(rev_X)\n",
    "        \n",
    "        X_v = self.embedding_v(X)\n",
    "        X_o = self.embedding_o(X)\n",
    "        \n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        \n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        \n",
    "        # c = attention_sum(alpha, beta, rev_x, rev_masks) # RETAIN\n",
    "        \n",
    "        # er = alpha * (beta @ (X_v + X_o)).T # INPREM\n",
    "        \n",
    "        logits = self.fc(c)\n",
    "        probs = self.sigmoid(logits)\n",
    "                      \n",
    "        out = F.softmax(probs.squeeze())\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "retain = RETAIN(num_codes = len(types))\n",
    "retain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce4102",
   "metadata": {},
   "source": [
    "## INPREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "17b78ac0-1bc5-46fa-98ac-83ca4cd3695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.sparsemax = Sparsemax()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, g):\n",
    "        \n",
    "        y = self.lin(g)\n",
    "        sparse_max = self.sparsemax(y)\n",
    "        soft_max = self.softmax(y)\n",
    "        \n",
    "        out = (sparse_max + soft_max) / 2\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BetaAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \n",
    "        y = self.lin(h)\n",
    "        out = torch.tanh(y)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, hidden_dim=256, num_attention_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc_multi = nn.Linear(num_attention_layers * hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(hidden_dim, hidden_dim, 1)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv1d(hidden_dim, hidden_dim, 1)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, E):\n",
    "        \n",
    "        Q = self.fc_q(E)\n",
    "        K = self.fc_k(E)\n",
    "        V = self.fc_v(E)\n",
    "        \n",
    "        attn = V * F.softmax((Q * K) / math.sqrt(self.hidden_dim))\n",
    "        \n",
    "        x = self.fc_multi(torch.concat((attn, attn), dim=2))\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "74799646-67a2-4780-a4e2-8eabedce15f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INPREM(\n",
       "  (embedding_v): Linear(in_features=1071, out_features=256, bias=False)\n",
       "  (embedding_o): Linear(in_features=1, out_features=256, bias=False)\n",
       "  (att_a): AlphaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (sparsemax): Sparsemax(dim=-1)\n",
       "    (softmax): Softmax(dim=None)\n",
       "  )\n",
       "  (att_b): BetaAttention(\n",
       "    (lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (multi_attn): MultiHeadAttention(\n",
       "    (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_multi): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=1071, bias=True)\n",
       "  (do): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class INPREM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_visits, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # self.embedding_v = nn.Embedding(num_codes, embedding_dim)\n",
    "        # self.embedding_o = nn.Embedding(num_codes, embedding_dim)\n",
    "        \n",
    "        self.embedding_v = nn.Linear(num_codes, embedding_dim, bias=False)\n",
    "        self.embedding_o = nn.Linear(1, embedding_dim, bias=False)\n",
    "        \n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        \n",
    "        self.multi_attn = MultiHeadAttention(num_codes, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, num_codes)\n",
    "        \n",
    "        self.do = nn.Dropout(.5)\n",
    "    \n",
    "    def forward(self, X, O):\n",
    "        \n",
    "        X = X.type(torch.FloatTensor)\n",
    "        O = O.type(torch.FloatTensor)\n",
    "        \n",
    "        O = O.unsqueeze(dim=2)\n",
    "        \n",
    "        E_o = O.unsqueeze(dim=1)\n",
    "        \n",
    "        # Get E_v\n",
    "        E_v = self.embedding_v(X)\n",
    "        E_o = self.embedding_o(O)\n",
    "        \n",
    "        E = torch.add(E_o,E_v)\n",
    "        \n",
    "        H =  self.multi_attn(E)\n",
    "        H =  self.multi_attn(H)\n",
    "        \n",
    "        beta = self.att_b(H)\n",
    "        alpha = self.att_a(H)\n",
    "        \n",
    "        E_r = (alpha * (beta * E))\n",
    "        \n",
    "        print(\"E_r: \", E_r.shape)\n",
    "        \n",
    "        out = self.fc(E_r)\n",
    "        print(\"Out: \", out.shape)\n",
    "                      \n",
    "        out = F.softmax(out, dim=2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "# load the model here\n",
    "model = INPREM(len(types), max_num_visits)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5e895-e644-4790-9a3e-51d2ffdfc451",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1ee84c93-6357-4d93-b612-b7fde9d30c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, device=None):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    \n",
    "    for DATA in dataloader:\n",
    "        y_logit = model(DATA)\n",
    "\n",
    "        y_hat = (y_logit > 0.5).int()\n",
    "\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    \n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5aa54-d7b3-4983-a834-77c898657366",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e402cdc6-bb7b-4f0b-8efd-f6efff628b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for x, o, y in train_loader:\n",
    "        # for DATA, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = model(x, o)\n",
    "            torch.set_printoptions(threshold=6)\n",
    "            \n",
    "            print(y_hat[0])\n",
    "            \n",
    "            print(y_hat.shape)\n",
    "            print(y.shape)\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        \n",
    "        p, r, f, roc_auc = eval(model, val_loader)\n",
    "        \n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "        \n",
    "        # get performance data\n",
    "        print('The CPU usage over the last 5 seconds is: {}'.format(psutil.cpu_percent(5)))\n",
    "        print('RAM memory % used is: {} of {}'.format(psutil.virtual_memory()[2], psutil.virtual_memory()[0]))\n",
    "        print('GPU statistics: {}'.format(torch.cuda.mem_get_info()))\n",
    "        \n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630845b1-f35a-4f28-93d3-593bfa3e9993",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35875830-7833-4907-9b0e-721957ac0db9",
   "metadata": {},
   "source": [
    "For training all approaches, we use Adam with the batch size of 32 and the learning rate of 0.0005. The weight decay is set to ùúÜ = 0.0001 and the dropout rate is set to 0.5 for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3ffaa109-7d45-4a3e-b9bb-0c9748a12770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_r:  torch.Size([32, 40, 256])\n",
      "tensor([[-0.0467, -0.0249,  0.0181,  ...,  0.0343,  0.0315, -0.0263],\n",
      "        [-0.0526, -0.0233,  0.0254,  ...,  0.0317,  0.0297, -0.0223],\n",
      "        [-0.0526, -0.0233,  0.0254,  ...,  0.0317,  0.0297, -0.0223],\n",
      "        ...,\n",
      "        [-0.0526, -0.0233,  0.0254,  ...,  0.0317,  0.0297, -0.0223],\n",
      "        [-0.0526, -0.0233,  0.0254,  ...,  0.0317,  0.0297, -0.0223],\n",
      "        [-0.0526, -0.0233,  0.0254,  ...,  0.0317,  0.0297, -0.0223]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([32, 40, 1071])\n",
      "torch.Size([32, 1071])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/85vy06cn6b13gcm4rcvz9_fw0000gq/T/ipykernel_2367/3290423086.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attn = V * F.softmax((Q * K) / math.sqrt(self.hidden_dim))\n",
      "/var/folders/jc/85vy06cn6b13gcm4rcvz9_fw0000gq/T/ipykernel_2367/3290423086.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max = self.softmax(y)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32, 1071])) that is different to the input size (torch.Size([32, 40, 1071])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [194]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     10\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [193]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py:603\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py:2906\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2904\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 2906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   2909\u001b[0m     )\n\u001b[1;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2912\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32, 1071])) that is different to the input size (torch.Size([32, 40, 1071])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = INPREM(len(types), max_num_visits)\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "train(model, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b9314-e48e-41f1-bfec-a23381f59b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bbc121b-8d7d-4d90-84b8-eed2a485ace3",
   "metadata": {},
   "source": [
    "## Abblations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c088435-9f4e-47e4-9fd2-3743022347db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0f9c2-bc37-46de-bc94-36feba5fea66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aa2f7-8c59-4eb2-b373-05eb7da9729b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

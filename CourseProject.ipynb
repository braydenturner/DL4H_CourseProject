{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3a4738-2e3b-4230-a760-5aa18887dbdd",
   "metadata": {},
   "source": [
    "# Deep Learning For Healthcare Course Project: INPREM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bee9d-8169-434e-826e-2e4c9b29c687",
   "metadata": {},
   "source": [
    "https://www.kdd.org/kdd2020/accepted-papers/view/inprem-an-interpretable-and-trustworthy-predictive-model-for-healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47c1bd-157c-444a-be21-ebc3cdbd4645",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5063b31c-2e09-4c90-933d-34354f6fb5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sparsemax in /opt/conda/lib/python3.8/site-packages (0.1.9)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from sparsemax) (1.11.0a0+bfe5ad2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->sparsemax) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3dc9044-b4f4-4504-99a1-041a4d89e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsemax import Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0dc863de-4dea-4fe9-b07c-f02da06a2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "use_demo = False\n",
    "if use_demo:\n",
    "    DATA_PATH = \"demodata/\" # work with open source data\n",
    "else:\n",
    "    DATA_PATH = \"data/\" # work with PATIENT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9b317c7b-cdec-477b-ab90-f5004336588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMISSIONS.csv      DIAGNOSES_ICD.csv   D_ICD_DIAGNOSES.csv ICUSTAYS.csv\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59820ba-ebce-486b-8c10-013ffb6615cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafcd45-05f9-4855-89dc-c9f3afe2a3c9",
   "metadata": {},
   "source": [
    "For example, SUBJECT_ID refers to a unique patient, HADM_ID refers to a unique admission to the hospital, and ICUSTAY_ID refers to a unique admission to an intensive care unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "78d39bac-2632-4d00-9982-d3d5c915774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag_icd (651047 lines):\n",
      "    hadm_id icd9_code\n",
      "0   172335     40301\n",
      "1   172335       486\n",
      "2   172335     58281\n",
      "3   172335      5855\n",
      "4   172335      4254\n",
      "\n",
      "icustays (61532 lines):\n",
      "    subject_id  hadm_id  icustay_id              outtime\n",
      "0         268   110404      280836  2198-02-18 05:26:11\n",
      "1         269   106296      206613  2170-11-08 17:46:57\n",
      "2         270   188028      220345  2128-06-27 12:32:29\n",
      "3         271   173727      249196  2120-08-10 00:39:04\n",
      "4         272   164716      210407  2186-12-27 12:01:13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def convert_datetime_to_day(df):\n",
    "    temp = pd.DataFrame()\n",
    "    temp[\"date\"] = pd.to_datetime(df['outtime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(temp['date'].dt.year) + str(temp['date'].dt.month) + str(temp['date'].dt.day)\n",
    "\n",
    "diag_icd = load_dataset(os.path.join(DATA_PATH, 'DIAGNOSES_ICD.csv'))\n",
    "icd_descriptions = load_dataset(os.path.join(DATA_PATH, 'D_ICD_DIAGNOSES.csv'))\n",
    "icustays = load_dataset(os.path.join(DATA_PATH, 'ICUSTAYS.csv'))\n",
    "admissions = load_dataset(os.path.join(DATA_PATH, 'ADMISSIONS.csv'))\n",
    "\n",
    "diag_icd = diag_icd.rename(columns={\"hadm_id\".upper(): \"hadm_id\", \"icd9_code\".upper(): \"icd9_code\"})\n",
    "icustays = icustays.rename(columns={\"subject_id\".upper(): \"subject_id\", \"hadm_id\".upper(): \"hadm_id\", \"icustay_id\".upper(): \"icustay_id\", \"outtime\".upper(): \"outtime\"})\n",
    "\n",
    "diag_icd = diag_icd[[\"hadm_id\", \"icd9_code\"]]\n",
    "icustays = icustays[[\"subject_id\", \"hadm_id\", \"icustay_id\", \"outtime\"]]\n",
    "\n",
    "\n",
    "print(f\"diag_icd ({len(diag_icd)} lines):\\n\", diag_icd.head(), end=\"\\n\\n\")\n",
    "print(f\"icustays ({len(icustays)} lines):\\n\", icustays.head(), end=\"\\n\\n\")\n",
    "# print(f\"admissions ({admissions.size} lines):\\n\", admissions.head(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b12ade38-be99-44c0-9fb7-e5c2dc2b1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_df (705921 lines):\n",
      "   icd9_code  subject_id  icustay_id              outtime\n",
      "0     40301         109      262652  2141-09-22 21:44:50\n",
      "1       486         109      262652  2141-09-22 21:44:50\n",
      "2     58281         109      262652  2141-09-22 21:44:50\n",
      "3      5855         109      262652  2141-09-22 21:44:50\n",
      "4      4254         109      262652  2141-09-22 21:44:50\n"
     ]
    }
   ],
   "source": [
    "joined_df = pd.merge(diag_icd, icustays, how='inner', on='hadm_id')[[\"icd9_code\", \"subject_id\", \"icustay_id\", \"outtime\"]]\n",
    "print(f\"joined_df ({len(joined_df)} lines):\\n\", joined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f3438-5929-4f94-90fd-50e2af5b69b3",
   "metadata": {},
   "source": [
    "Convert y in to category labels. If (3, 4 ,5) always use left 3. If it starts with E, use (Exxx). If it starts with V, use (Vxx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6c847451-714f-4bd3-b80a-a37a497105c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8755 patients\n"
     ]
    }
   ],
   "source": [
    "def convert_codes(codes):\n",
    "    out = []\n",
    "    for code in codes:\n",
    "        code = str(code)\n",
    "        if code[0] == \"E\":\n",
    "            c = code[:4]\n",
    "        else:\n",
    "            c = code[:3]\n",
    "            out.append(c)\n",
    "        unique_codes.add(c)\n",
    "            \n",
    "    return out\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "unique_codes = set()\n",
    "\n",
    "for name, patient in joined_df.sort_values(\"outtime\").groupby([\"subject_id\"]):\n",
    "    visits = []\n",
    "    for _, visit in patient.groupby([\"icustay_id\"]):\n",
    "        codes = visit[\"icd9_code\"].tolist()\n",
    "        codes = convert_codes(codes)\n",
    "        visits.append(codes)\n",
    "    if len(visits) >= 2:\n",
    "        x, y_ = visits[:-1], visits[-1]\n",
    "        X.append(x)\n",
    "        y.append(y_)\n",
    "        \n",
    "print(f\"Using {len(X)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "79702804-810c-41c6-9381-fb1681ac3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X) == len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "69f80811-2ae9-44bf-81bc-77b08f9edabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['003',\n",
       " '004',\n",
       " '005',\n",
       " '007',\n",
       " '008',\n",
       " '009',\n",
       " '010',\n",
       " '011',\n",
       " '012',\n",
       " '013',\n",
       " '014',\n",
       " '015',\n",
       " '018',\n",
       " '021',\n",
       " '023',\n",
       " '027',\n",
       " '030',\n",
       " '031',\n",
       " '032',\n",
       " '033',\n",
       " '034',\n",
       " '035',\n",
       " '036',\n",
       " '038',\n",
       " '039',\n",
       " '040',\n",
       " '041',\n",
       " '042',\n",
       " '045',\n",
       " '046',\n",
       " '047',\n",
       " '048',\n",
       " '049',\n",
       " '052',\n",
       " '053',\n",
       " '054',\n",
       " '057',\n",
       " '058',\n",
       " '062',\n",
       " '066',\n",
       " '070',\n",
       " '075',\n",
       " '077',\n",
       " '078',\n",
       " '079',\n",
       " '082',\n",
       " '083',\n",
       " '084',\n",
       " '085',\n",
       " '086',\n",
       " '088',\n",
       " '090',\n",
       " '091',\n",
       " '093',\n",
       " '094',\n",
       " '096',\n",
       " '097',\n",
       " '098',\n",
       " '099',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '125',\n",
       " '127',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '175',\n",
       " '176',\n",
       " '179',\n",
       " '180',\n",
       " '182',\n",
       " '183',\n",
       " '184',\n",
       " '185',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '190',\n",
       " '191',\n",
       " '192',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '196',\n",
       " '197',\n",
       " '198',\n",
       " '199',\n",
       " '200',\n",
       " '201',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '220',\n",
       " '221',\n",
       " '223',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '254',\n",
       " '255',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '265',\n",
       " '266',\n",
       " '267',\n",
       " '268',\n",
       " '269',\n",
       " '270',\n",
       " '271',\n",
       " '272',\n",
       " '273',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '279',\n",
       " '280',\n",
       " '281',\n",
       " '282',\n",
       " '283',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '289',\n",
       " '290',\n",
       " '291',\n",
       " '292',\n",
       " '293',\n",
       " '294',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " '300',\n",
       " '301',\n",
       " '302',\n",
       " '303',\n",
       " '304',\n",
       " '305',\n",
       " '306',\n",
       " '307',\n",
       " '308',\n",
       " '309',\n",
       " '310',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '316',\n",
       " '317',\n",
       " '318',\n",
       " '319',\n",
       " '320',\n",
       " '321',\n",
       " '322',\n",
       " '323',\n",
       " '324',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '330',\n",
       " '331',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '349',\n",
       " '350',\n",
       " '351',\n",
       " '352',\n",
       " '353',\n",
       " '354',\n",
       " '355',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '359',\n",
       " '360',\n",
       " '361',\n",
       " '362',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '369',\n",
       " '370',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '374',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '378',\n",
       " '379',\n",
       " '380',\n",
       " '381',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '387',\n",
       " '388',\n",
       " '389',\n",
       " '390',\n",
       " '391',\n",
       " '393',\n",
       " '394',\n",
       " '395',\n",
       " '396',\n",
       " '397',\n",
       " '398',\n",
       " '401',\n",
       " '402',\n",
       " '403',\n",
       " '404',\n",
       " '405',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '424',\n",
       " '425',\n",
       " '426',\n",
       " '427',\n",
       " '428',\n",
       " '429',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '434',\n",
       " '435',\n",
       " '436',\n",
       " '437',\n",
       " '438',\n",
       " '440',\n",
       " '441',\n",
       " '442',\n",
       " '443',\n",
       " '444',\n",
       " '445',\n",
       " '446',\n",
       " '447',\n",
       " '448',\n",
       " '449',\n",
       " '451',\n",
       " '452',\n",
       " '453',\n",
       " '454',\n",
       " '455',\n",
       " '456',\n",
       " '457',\n",
       " '458',\n",
       " '459',\n",
       " '460',\n",
       " '461',\n",
       " '462',\n",
       " '463',\n",
       " '464',\n",
       " '465',\n",
       " '466',\n",
       " '470',\n",
       " '471',\n",
       " '472',\n",
       " '473',\n",
       " '474',\n",
       " '475',\n",
       " '477',\n",
       " '478',\n",
       " '480',\n",
       " '481',\n",
       " '482',\n",
       " '483',\n",
       " '484',\n",
       " '485',\n",
       " '486',\n",
       " '487',\n",
       " '488',\n",
       " '490',\n",
       " '491',\n",
       " '492',\n",
       " '493',\n",
       " '494',\n",
       " '495',\n",
       " '496',\n",
       " '500',\n",
       " '501',\n",
       " '502',\n",
       " '506',\n",
       " '507',\n",
       " '508',\n",
       " '510',\n",
       " '511',\n",
       " '512',\n",
       " '513',\n",
       " '514',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '520',\n",
       " '521',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '526',\n",
       " '527',\n",
       " '528',\n",
       " '529',\n",
       " '530',\n",
       " '531',\n",
       " '532',\n",
       " '533',\n",
       " '534',\n",
       " '535',\n",
       " '536',\n",
       " '537',\n",
       " '538',\n",
       " '539',\n",
       " '540',\n",
       " '541',\n",
       " '542',\n",
       " '543',\n",
       " '550',\n",
       " '551',\n",
       " '552',\n",
       " '553',\n",
       " '555',\n",
       " '556',\n",
       " '557',\n",
       " '558',\n",
       " '560',\n",
       " '562',\n",
       " '564',\n",
       " '565',\n",
       " '566',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '570',\n",
       " '571',\n",
       " '572',\n",
       " '573',\n",
       " '574',\n",
       " '575',\n",
       " '576',\n",
       " '577',\n",
       " '578',\n",
       " '579',\n",
       " '580',\n",
       " '581',\n",
       " '582',\n",
       " '583',\n",
       " '584',\n",
       " '585',\n",
       " '586',\n",
       " '587',\n",
       " '588',\n",
       " '589',\n",
       " '590',\n",
       " '591',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '595',\n",
       " '596',\n",
       " '597',\n",
       " '598',\n",
       " '599',\n",
       " '600',\n",
       " '601',\n",
       " '602',\n",
       " '603',\n",
       " '604',\n",
       " '605',\n",
       " '607',\n",
       " '608',\n",
       " '610',\n",
       " '611',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '619',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '623',\n",
       " '624',\n",
       " '625',\n",
       " '626',\n",
       " '627',\n",
       " '628',\n",
       " '629',\n",
       " '632',\n",
       " '633',\n",
       " '634',\n",
       " '635',\n",
       " '639',\n",
       " '641',\n",
       " '642',\n",
       " '643',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '651',\n",
       " '652',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '657',\n",
       " '658',\n",
       " '659',\n",
       " '660',\n",
       " '661',\n",
       " '663',\n",
       " '664',\n",
       " '665',\n",
       " '666',\n",
       " '668',\n",
       " '669',\n",
       " '670',\n",
       " '671',\n",
       " '672',\n",
       " '673',\n",
       " '674',\n",
       " '677',\n",
       " '680',\n",
       " '681',\n",
       " '682',\n",
       " '683',\n",
       " '684',\n",
       " '685',\n",
       " '686',\n",
       " '690',\n",
       " '691',\n",
       " '692',\n",
       " '693',\n",
       " '694',\n",
       " '695',\n",
       " '696',\n",
       " '697',\n",
       " '698',\n",
       " '700',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '704',\n",
       " '705',\n",
       " '706',\n",
       " '707',\n",
       " '708',\n",
       " '709',\n",
       " '710',\n",
       " '711',\n",
       " '712',\n",
       " '713',\n",
       " '714',\n",
       " '715',\n",
       " '716',\n",
       " '717',\n",
       " '718',\n",
       " '719',\n",
       " '720',\n",
       " '721',\n",
       " '722',\n",
       " '723',\n",
       " '724',\n",
       " '725',\n",
       " '726',\n",
       " '727',\n",
       " '728',\n",
       " '729',\n",
       " '730',\n",
       " '731',\n",
       " '732',\n",
       " '733',\n",
       " '734',\n",
       " '735',\n",
       " '736',\n",
       " '737',\n",
       " '738',\n",
       " '740',\n",
       " '741',\n",
       " '742',\n",
       " '743',\n",
       " '744',\n",
       " '745',\n",
       " '746',\n",
       " '747',\n",
       " '748',\n",
       " '749',\n",
       " '750',\n",
       " '751',\n",
       " '752',\n",
       " '753',\n",
       " '754',\n",
       " '755',\n",
       " '756',\n",
       " '757',\n",
       " '758',\n",
       " '759',\n",
       " '760',\n",
       " '761',\n",
       " '762',\n",
       " '763',\n",
       " '764',\n",
       " '765',\n",
       " '766',\n",
       " '767',\n",
       " '768',\n",
       " '769',\n",
       " '770',\n",
       " '771',\n",
       " '772',\n",
       " '773',\n",
       " '774',\n",
       " '775',\n",
       " '776',\n",
       " '777',\n",
       " '778',\n",
       " '779',\n",
       " '780',\n",
       " '781',\n",
       " '782',\n",
       " '783',\n",
       " '784',\n",
       " '785',\n",
       " '786',\n",
       " '787',\n",
       " '788',\n",
       " '789',\n",
       " '790',\n",
       " '791',\n",
       " '792',\n",
       " '793',\n",
       " '794',\n",
       " '795',\n",
       " '796',\n",
       " '798',\n",
       " '799',\n",
       " '800',\n",
       " '801',\n",
       " '802',\n",
       " '803',\n",
       " '804',\n",
       " '805',\n",
       " '806',\n",
       " '807',\n",
       " '808',\n",
       " '810',\n",
       " '811',\n",
       " '812',\n",
       " '813',\n",
       " '814',\n",
       " '815',\n",
       " '816',\n",
       " '817',\n",
       " '819',\n",
       " '820',\n",
       " '821',\n",
       " '822',\n",
       " '823',\n",
       " '824',\n",
       " '825',\n",
       " '826',\n",
       " '828',\n",
       " '830',\n",
       " '831',\n",
       " '832',\n",
       " '833',\n",
       " '834',\n",
       " '835',\n",
       " '836',\n",
       " '837',\n",
       " '838',\n",
       " '839',\n",
       " '840',\n",
       " '841',\n",
       " '842',\n",
       " '843',\n",
       " '844',\n",
       " '845',\n",
       " '846',\n",
       " '847',\n",
       " '848',\n",
       " '850',\n",
       " '851',\n",
       " '852',\n",
       " '853',\n",
       " '854',\n",
       " '860',\n",
       " '861',\n",
       " '862',\n",
       " '863',\n",
       " '864',\n",
       " '865',\n",
       " '866',\n",
       " '867',\n",
       " '868',\n",
       " '869',\n",
       " '870',\n",
       " '871',\n",
       " '872',\n",
       " '873',\n",
       " '874',\n",
       " '875',\n",
       " '876',\n",
       " '877',\n",
       " '878',\n",
       " '879',\n",
       " '880',\n",
       " '881',\n",
       " '882',\n",
       " '883',\n",
       " '884',\n",
       " '885',\n",
       " '886',\n",
       " '887',\n",
       " '890',\n",
       " '891',\n",
       " '892',\n",
       " '893',\n",
       " '894',\n",
       " '896',\n",
       " '897',\n",
       " '900',\n",
       " '901',\n",
       " '902',\n",
       " '903',\n",
       " '904',\n",
       " '905',\n",
       " '906',\n",
       " '907',\n",
       " '908',\n",
       " '909',\n",
       " '910',\n",
       " '911',\n",
       " '912',\n",
       " '913',\n",
       " '914',\n",
       " '915',\n",
       " '916',\n",
       " '917',\n",
       " '918',\n",
       " '919',\n",
       " '920',\n",
       " '921',\n",
       " '922',\n",
       " '923',\n",
       " '924',\n",
       " '925',\n",
       " '926',\n",
       " '927',\n",
       " '928',\n",
       " '930',\n",
       " '932',\n",
       " '933',\n",
       " '934',\n",
       " '935',\n",
       " '936',\n",
       " '937',\n",
       " '938',\n",
       " '939',\n",
       " '941',\n",
       " '942',\n",
       " '943',\n",
       " '944',\n",
       " '945',\n",
       " '946',\n",
       " '947',\n",
       " '948',\n",
       " '950',\n",
       " '951',\n",
       " '952',\n",
       " '953',\n",
       " '954',\n",
       " '955',\n",
       " '956',\n",
       " '957',\n",
       " '958',\n",
       " '959',\n",
       " '960',\n",
       " '961',\n",
       " '962',\n",
       " '963',\n",
       " '964',\n",
       " '965',\n",
       " '966',\n",
       " '967',\n",
       " '968',\n",
       " '969',\n",
       " '970',\n",
       " '971',\n",
       " '972',\n",
       " '973',\n",
       " '974',\n",
       " '975',\n",
       " '976',\n",
       " '977',\n",
       " '980',\n",
       " '982',\n",
       " '983',\n",
       " '985',\n",
       " '986',\n",
       " '987',\n",
       " '988',\n",
       " '989',\n",
       " '990',\n",
       " '991',\n",
       " '992',\n",
       " '994',\n",
       " '995',\n",
       " '996',\n",
       " '997',\n",
       " '998',\n",
       " '999',\n",
       " 'E00',\n",
       " 'E01',\n",
       " 'E02',\n",
       " 'E03',\n",
       " 'E80',\n",
       " 'E81',\n",
       " 'E82',\n",
       " 'E83',\n",
       " 'E84',\n",
       " 'E85',\n",
       " 'E86',\n",
       " 'E87',\n",
       " 'E88',\n",
       " 'E89',\n",
       " 'E90',\n",
       " 'E91',\n",
       " 'E92',\n",
       " 'E93',\n",
       " 'E94',\n",
       " 'E95',\n",
       " 'E96',\n",
       " 'E97',\n",
       " 'E98',\n",
       " 'E99',\n",
       " 'V01',\n",
       " 'V02',\n",
       " 'V03',\n",
       " 'V04',\n",
       " 'V05',\n",
       " 'V06',\n",
       " 'V07',\n",
       " 'V08',\n",
       " 'V09',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V28',\n",
       " 'V29',\n",
       " 'V30',\n",
       " 'V31',\n",
       " 'V32',\n",
       " 'V33',\n",
       " 'V34',\n",
       " 'V40',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V74',\n",
       " 'V76',\n",
       " 'V78',\n",
       " 'V81',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'nan']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(unique_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f406311-1f41-41f0-86dc-2d09a286533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD 9 Codes for Binary Classification\n",
    "diabetes = (\"Diabetes\", \"250.xx\")\n",
    "heart failure = (\"Heary Failure\", \"428.xx\")\n",
    "chronic_kidney_disease = (\"Chronic Kidney Disease\", \"585.xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da5af8",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7071d",
   "metadata": {},
   "source": [
    "For each task, we randomly split each dataset into training, validation, and testing sets five times in a 75:10:15 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1d8b0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.75\n",
    "\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(X, y, train_size=0.75)\n",
    "\n",
    "test_size = 0.6 # (valid is 10% of remaining 25%, test is 15% of remaining 25%)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf4afc-5be4-4ac2-831a-60d6eaa52c10",
   "metadata": {},
   "source": [
    "## Build Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3895e91d-962b-40de-852d-afb900d64b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])\n",
    "        \n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_valid, y_valid)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f5e7e-a159-45df-a748-97b3c904c3fc",
   "metadata": {},
   "source": [
    "## Load the Data (DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46fda0-2d1c-4529-a191-8f3f190e1b57",
   "metadata": {},
   "source": [
    "For each task, we randomly split each dataset into training, validation, and testing sets five times in a 75:10:15 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "784b31cb-ac66-46e2-80dd-b0d9813a973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patients) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    y = torch.zeros((num_patients, , dtype=torch.float)\n",
    "    \n",
    "    for label in labels:\n",
    "    \n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        count = 0\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            visit_len = len(visit)\n",
    "            \n",
    "            x[i_patient][j_visit][:visit_len] = torch.tensor(visit)\n",
    "            masks[i_patient][j_visit][:visit_len] = torch.ones((visit_len),dtype=torch.bool)\n",
    "            count+=1\n",
    "            \n",
    "        reverse_x = x[i_patient][:count]\n",
    "        reverse_mask = masks[i_patient][:count]\n",
    "        \n",
    "        rev_x[i_patient][:count] = torch.flip(reverse_x, [0])\n",
    "        rev_masks[i_patient][:count] = torch.flip(reverse_mask, [0])\n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e8c87ac3-0929-4112-a412-ea39a870ccce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [163]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x, masks, rev_x, rev_masks, y \u001b[38;5;241m=\u001b[39m \u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [160]\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    data: a list of samples fetched from `CustomDataset`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    using: `sequences, labels = zip(*data)`\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m sequences, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 19\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m num_patients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequences)\n\u001b[1;32m     22\u001b[0m num_visits \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(patient) \u001b[38;5;28;01mfor\u001b[39;00m patient \u001b[38;5;129;01min\u001b[39;00m sequences]\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "x, masks, rev_x, rev_masks, y = collate_fn(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c2d03a1e-7bd8-467c-b155-b1ab699ce8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, test_loader, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_loader, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c7b45-5f82-4852-8ad2-a7775c775d75",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee0171-82a3-4f49-8baa-fef7e1f5efee",
   "metadata": {},
   "source": [
    "We treat the medical events taking place in EHR as medical codes, which are denoted as $c_{1}, c_{2},... c_{|C|}$ ∈ 𝐶, where |𝐶| is the total number of unique medical codes.\n",
    "\n",
    "One specific patient consist of a sequence of visits $v_{1}, v_{2},... v_{T}$ where we denote the number of visits in total as T.\n",
    "\n",
    "Each visit contains a subset of medical codes, and we denote each visit as a binary vector  $v_{t} ∈ \\{0, 1\\}_{|C|}$, where the 𝑖-th element is set to 1 if the 𝑡-th visit contains the medical code $c_{i}$, otherwise 0. The visits  $v_{1}, v_{2},... v_{T}$ are stacked to form an input matrix $X ∈ \\{0, 1\\}^{|C|xT}$ , which we use as the input for the network\n",
    "\n",
    "$E_{v} = {W}_{v}X$\n",
    "\n",
    "$E_{o} = {W}_{o}O$\n",
    "\n",
    "$E_{r} = \\alpha(\\beta \\odot (E_{v}+E_{o}))^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b78ac0-1bc5-46fa-98ac-83ca4cd3695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.sparsemax = Sparsemax(dim=-1)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \n",
    "        y = self.a_att(g)\n",
    "        sparse_max = self.sparsemax(y)\n",
    "        soft_max = self.softmax(y)\n",
    "        \n",
    "        out = (sparse_max + soft_max) / 2\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \n",
    "        y = self.b_att(h)\n",
    "        out = torch.tanh(y)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74799646-67a2-4780-a4e2-8eabedce15f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_455/825688894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# load the model here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINPREM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'types' is not defined"
     ]
    }
   ],
   "source": [
    "class INPREM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_v = nn.Embedding(num_codes, embedding_dim)\n",
    "        self.embedding_o = nn.Embedding(num_codes, embedding_dim)\n",
    "        \n",
    "        self.att_a = AlphaAttention(embedding_dim)\n",
    "        \n",
    "        self.att_b = BetaAttention(embedding_dim)\n",
    "        \n",
    "        self.do = nn.Dropout(.5)\n",
    "    \n",
    "    def forward(self, X):\n",
    "    \n",
    "        # Pass through embedding\n",
    "        ev = self.embedding_v(X)\n",
    "        eo = self.embedding_o(o)\n",
    "        \n",
    "        er = self.att_a * (self.att_b @ (ev + eo)).T # double check this\n",
    "        \n",
    "        # Softmax\n",
    "        out = F.softmax(x)\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "model = INPREM(num_codes = len(types))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5e895-e644-4790-9a3e-51d2ffdfc451",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee84c93-6357-4d93-b612-b7fde9d30c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, device=None):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    \n",
    "    for DATA in dataloader:\n",
    "        y_logit = model(DATA)\n",
    "\n",
    "        y_hat = (y_logit > 0.5).int()\n",
    "\n",
    "        y_score = torch.cat((y_score,  y_logit.detach().to('cpu')), dim=0)\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    \n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5aa54-d7b3-4983-a834-77c898657366",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402cdc6-bb7b-4f0b-8efd-f6efff628b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        for DATA, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        \n",
    "        p, r, f, roc_auc = eval(model, val_loader)\n",
    "        \n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'.format(epoch+1, p, r, f, roc_auc))\n",
    "        \n",
    "    return round(roc_auc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630845b1-f35a-4f28-93d3-593bfa3e9993",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35875830-7833-4907-9b0e-721957ac0db9",
   "metadata": {},
   "source": [
    "For training all approaches, we use Adam with the batch size of 32 and the learning rate of 0.0005. The weight decay is set to 𝜆 = 0.0001 and the dropout rate is set to 0.5 for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffaa109-7d45-4a3e-b9bb-0c9748a12770",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMPREM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mIMPREM\u001b[49m(num_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(types))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load the loss function\u001b[39;00m\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IMPREM' is not defined"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = IMPREM(num_codes = len(types))\n",
    "\n",
    "# load the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# load the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "train(model, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b9314-e48e-41f1-bfec-a23381f59b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bbc121b-8d7d-4d90-84b8-eed2a485ace3",
   "metadata": {},
   "source": [
    "## Abblations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c088435-9f4e-47e4-9fd2-3743022347db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
